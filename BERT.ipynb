{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFk8t7woCK-S",
        "outputId": "c5e99445-dda2-4dbf-85ee-f0f66ceac176"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/S17"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve8SyS73CO1Z",
        "outputId": "15062677-9c0d-4410-d724-10714f2d9dbd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/S17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b_WTfa3CB58U",
        "outputId": "8e3a910c-38dc-42d8-a670-f48d3e2597ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing..\n",
            "loading text...\n",
            "tokenizing sentences...\n",
            "creating/loading vocab...\n",
            "creating dataset...\n",
            "initializing model...\n",
            "initializing optimizer and loss...\n",
            "training...\n",
            "it: 0  | loss 10.24  | Δw: 1.136\n",
            "it: 10  | loss 9.58  | Δw: 0.568\n",
            "it: 20  | loss 9.4  | Δw: 0.373\n",
            "it: 30  | loss 9.21  | Δw: 0.301\n",
            "it: 40  | loss 9.1  | Δw: 0.245\n",
            "it: 50  | loss 8.91  | Δw: 0.219\n",
            "it: 60  | loss 8.75  | Δw: 0.207\n",
            "it: 70  | loss 8.62  | Δw: 0.187\n",
            "it: 80  | loss 8.47  | Δw: 0.183\n",
            "it: 90  | loss 8.34  | Δw: 0.171\n",
            "it: 100  | loss 8.15  | Δw: 0.162\n",
            "it: 110  | loss 8.02  | Δw: 0.161\n",
            "it: 120  | loss 7.87  | Δw: 0.162\n",
            "it: 130  | loss 7.72  | Δw: 0.15\n",
            "it: 140  | loss 7.6  | Δw: 0.141\n",
            "it: 150  | loss 7.47  | Δw: 0.138\n",
            "it: 160  | loss 7.37  | Δw: 0.143\n",
            "it: 170  | loss 7.28  | Δw: 0.132\n",
            "it: 180  | loss 7.21  | Δw: 0.135\n",
            "it: 190  | loss 7.03  | Δw: 0.133\n",
            "it: 200  | loss 7.03  | Δw: 0.129\n",
            "it: 210  | loss 6.92  | Δw: 0.139\n",
            "it: 220  | loss 6.8  | Δw: 0.134\n",
            "it: 230  | loss 6.78  | Δw: 0.134\n",
            "it: 240  | loss 6.73  | Δw: 0.134\n",
            "it: 250  | loss 6.71  | Δw: 0.145\n",
            "it: 260  | loss 6.66  | Δw: 0.14\n",
            "it: 270  | loss 6.61  | Δw: 0.138\n",
            "it: 280  | loss 6.62  | Δw: 0.142\n",
            "it: 290  | loss 6.63  | Δw: 0.144\n",
            "it: 300  | loss 6.54  | Δw: 0.16\n",
            "it: 310  | loss 6.44  | Δw: 0.159\n",
            "it: 320  | loss 6.38  | Δw: 0.168\n",
            "it: 330  | loss 6.49  | Δw: 0.18\n",
            "it: 340  | loss 6.5  | Δw: 0.179\n",
            "it: 350  | loss 6.42  | Δw: 0.212\n",
            "it: 360  | loss 6.36  | Δw: 0.212\n",
            "it: 370  | loss 6.36  | Δw: 0.215\n",
            "it: 380  | loss 6.43  | Δw: 0.218\n",
            "it: 390  | loss 6.28  | Δw: 0.245\n",
            "it: 400  | loss 6.4  | Δw: 0.253\n",
            "it: 410  | loss 6.33  | Δw: 0.276\n",
            "it: 420  | loss 6.36  | Δw: 0.284\n",
            "it: 430  | loss 6.37  | Δw: 0.304\n",
            "it: 440  | loss 6.34  | Δw: 0.363\n",
            "it: 450  | loss 6.37  | Δw: 0.366\n",
            "it: 460  | loss 6.34  | Δw: 0.385\n",
            "it: 470  | loss 6.32  | Δw: 0.462\n",
            "it: 480  | loss 6.31  | Δw: 0.508\n",
            "it: 490  | loss 6.32  | Δw: 0.47\n",
            "it: 500  | loss 6.26  | Δw: 0.486\n",
            "it: 510  | loss 6.32  | Δw: 0.561\n",
            "it: 520  | loss 6.24  | Δw: 0.526\n",
            "it: 530  | loss 6.38  | Δw: 0.553\n",
            "it: 540  | loss 6.23  | Δw: 0.62\n",
            "it: 550  | loss 6.34  | Δw: 0.58\n",
            "it: 560  | loss 6.28  | Δw: 0.617\n",
            "it: 570  | loss 6.26  | Δw: 0.737\n",
            "it: 580  | loss 6.32  | Δw: 0.708\n",
            "it: 590  | loss 6.35  | Δw: 0.699\n",
            "it: 600  | loss 6.25  | Δw: 0.746\n",
            "it: 610  | loss 6.39  | Δw: 0.777\n",
            "it: 620  | loss 6.31  | Δw: 0.799\n",
            "it: 630  | loss 6.24  | Δw: 0.817\n",
            "it: 640  | loss 6.27  | Δw: 0.887\n",
            "it: 650  | loss 6.33  | Δw: 0.915\n",
            "it: 660  | loss 6.33  | Δw: 0.848\n",
            "it: 670  | loss 6.25  | Δw: 0.891\n",
            "it: 680  | loss 6.28  | Δw: 0.96\n",
            "it: 690  | loss 6.25  | Δw: 1.051\n",
            "it: 700  | loss 6.22  | Δw: 1.029\n",
            "it: 710  | loss 6.26  | Δw: 1.024\n",
            "it: 720  | loss 6.19  | Δw: 1.034\n",
            "it: 730  | loss 6.18  | Δw: 1.158\n",
            "it: 740  | loss 6.3  | Δw: 1.121\n",
            "it: 750  | loss 6.16  | Δw: 1.141\n",
            "it: 760  | loss 6.18  | Δw: 1.205\n",
            "it: 770  | loss 6.25  | Δw: 1.214\n",
            "it: 780  | loss 6.17  | Δw: 1.278\n",
            "it: 790  | loss 6.13  | Δw: 1.257\n",
            "it: 800  | loss 6.22  | Δw: 1.208\n",
            "it: 810  | loss 6.24  | Δw: 1.321\n",
            "it: 820  | loss 6.22  | Δw: 1.316\n",
            "it: 830  | loss 6.17  | Δw: 1.275\n",
            "it: 840  | loss 6.14  | Δw: 1.327\n",
            "it: 850  | loss 6.2  | Δw: 1.338\n",
            "it: 860  | loss 6.11  | Δw: 1.299\n",
            "it: 870  | loss 6.17  | Δw: 1.356\n",
            "it: 880  | loss 6.07  | Δw: 1.457\n",
            "it: 890  | loss 6.2  | Δw: 1.399\n",
            "it: 900  | loss 6.14  | Δw: 1.39\n",
            "it: 910  | loss 6.23  | Δw: 1.519\n",
            "it: 920  | loss 6.01  | Δw: 1.54\n",
            "it: 930  | loss 6.1  | Δw: 1.524\n",
            "it: 940  | loss 6.1  | Δw: 1.535\n",
            "it: 950  | loss 6.18  | Δw: 1.514\n",
            "it: 960  | loss 6.12  | Δw: 1.469\n",
            "it: 970  | loss 6.16  | Δw: 1.454\n",
            "it: 980  | loss 6.09  | Δw: 1.634\n",
            "it: 990  | loss 6.14  | Δw: 1.609\n",
            "it: 1000  | loss 6.09  | Δw: 1.489\n",
            "it: 1010  | loss 6.16  | Δw: 1.596\n",
            "it: 1020  | loss 6.1  | Δw: 1.66\n",
            "it: 1030  | loss 6.22  | Δw: 1.622\n",
            "it: 1040  | loss 6.07  | Δw: 1.632\n",
            "it: 1050  | loss 6.16  | Δw: 1.638\n",
            "it: 1060  | loss 6.03  | Δw: 1.878\n",
            "it: 1070  | loss 6.08  | Δw: 1.711\n",
            "it: 1080  | loss 6.03  | Δw: 1.84\n",
            "it: 1090  | loss 6.01  | Δw: 1.675\n",
            "it: 1100  | loss 6.09  | Δw: 1.765\n",
            "it: 1110  | loss 5.99  | Δw: 1.829\n",
            "it: 1120  | loss 6.13  | Δw: 1.899\n",
            "it: 1130  | loss 6.0  | Δw: 1.895\n",
            "it: 1140  | loss 6.03  | Δw: 1.957\n",
            "it: 1150  | loss 6.17  | Δw: 2.011\n",
            "it: 1160  | loss 6.01  | Δw: 1.903\n",
            "it: 1170  | loss 6.07  | Δw: 1.994\n",
            "it: 1180  | loss 6.04  | Δw: 2.046\n",
            "it: 1190  | loss 6.05  | Δw: 2.032\n",
            "it: 1200  | loss 5.99  | Δw: 1.945\n",
            "it: 1210  | loss 6.0  | Δw: 2.228\n",
            "it: 1220  | loss 6.01  | Δw: 2.261\n",
            "it: 1230  | loss 5.98  | Δw: 2.175\n",
            "it: 1240  | loss 5.87  | Δw: 2.247\n",
            "it: 1250  | loss 6.02  | Δw: 2.231\n",
            "it: 1260  | loss 5.99  | Δw: 2.323\n",
            "it: 1270  | loss 5.96  | Δw: 2.361\n",
            "it: 1280  | loss 5.95  | Δw: 2.227\n",
            "it: 1290  | loss 6.07  | Δw: 2.259\n",
            "it: 1300  | loss 5.89  | Δw: 2.412\n",
            "it: 1310  | loss 5.98  | Δw: 2.425\n",
            "it: 1320  | loss 5.97  | Δw: 2.404\n",
            "it: 1330  | loss 6.03  | Δw: 2.506\n",
            "it: 1340  | loss 5.95  | Δw: 2.45\n",
            "it: 1350  | loss 5.9  | Δw: 2.435\n",
            "it: 1360  | loss 5.92  | Δw: 2.623\n",
            "it: 1370  | loss 5.9  | Δw: 2.43\n",
            "it: 1380  | loss 5.92  | Δw: 2.517\n",
            "it: 1390  | loss 5.94  | Δw: 2.811\n",
            "it: 1400  | loss 5.89  | Δw: 2.617\n",
            "it: 1410  | loss 5.85  | Δw: 2.607\n",
            "it: 1420  | loss 5.88  | Δw: 2.837\n",
            "it: 1430  | loss 5.88  | Δw: 2.948\n",
            "it: 1440  | loss 5.86  | Δw: 2.707\n",
            "it: 1450  | loss 5.88  | Δw: 2.668\n",
            "it: 1460  | loss 5.88  | Δw: 2.815\n",
            "it: 1470  | loss 5.94  | Δw: 2.933\n",
            "it: 1480  | loss 5.87  | Δw: 2.812\n",
            "it: 1490  | loss 5.99  | Δw: 2.861\n",
            "it: 1500  | loss 5.86  | Δw: 3.006\n",
            "it: 1510  | loss 5.88  | Δw: 2.968\n",
            "it: 1520  | loss 5.94  | Δw: 2.88\n",
            "it: 1530  | loss 5.91  | Δw: 2.962\n",
            "it: 1540  | loss 5.91  | Δw: 2.854\n",
            "it: 1550  | loss 5.83  | Δw: 2.921\n",
            "it: 1560  | loss 5.84  | Δw: 2.979\n",
            "it: 1570  | loss 5.85  | Δw: 3.032\n",
            "it: 1580  | loss 5.78  | Δw: 3.164\n",
            "it: 1590  | loss 5.72  | Δw: 3.089\n",
            "it: 1600  | loss 5.81  | Δw: 3.009\n",
            "it: 1610  | loss 5.72  | Δw: 3.038\n",
            "it: 1620  | loss 5.88  | Δw: 3.091\n",
            "it: 1630  | loss 5.7  | Δw: 3.573\n",
            "it: 1640  | loss 5.71  | Δw: 3.224\n",
            "it: 1650  | loss 5.68  | Δw: 3.162\n",
            "it: 1660  | loss 5.71  | Δw: 3.243\n",
            "it: 1670  | loss 5.72  | Δw: 3.236\n",
            "it: 1680  | loss 5.8  | Δw: 3.292\n",
            "it: 1690  | loss 5.76  | Δw: 3.326\n",
            "it: 1700  | loss 5.72  | Δw: 3.128\n",
            "it: 1710  | loss 5.62  | Δw: 3.218\n",
            "it: 1720  | loss 5.69  | Δw: 3.318\n",
            "it: 1730  | loss 5.73  | Δw: 3.421\n",
            "it: 1740  | loss 5.76  | Δw: 3.278\n",
            "it: 1750  | loss 5.75  | Δw: 3.359\n",
            "it: 1760  | loss 5.73  | Δw: 3.425\n",
            "it: 1770  | loss 5.83  | Δw: 3.554\n",
            "it: 1780  | loss 5.7  | Δw: 3.485\n",
            "it: 1790  | loss 5.68  | Δw: 3.584\n",
            "it: 1800  | loss 5.67  | Δw: 3.975\n",
            "it: 1810  | loss 5.7  | Δw: 3.763\n",
            "it: 1820  | loss 5.71  | Δw: 3.555\n",
            "it: 1830  | loss 5.67  | Δw: 3.534\n",
            "it: 1840  | loss 5.7  | Δw: 3.555\n",
            "it: 1850  | loss 5.63  | Δw: 3.56\n",
            "it: 1860  | loss 5.71  | Δw: 3.55\n",
            "it: 1870  | loss 5.7  | Δw: 3.573\n",
            "it: 1880  | loss 5.63  | Δw: 3.691\n",
            "it: 1890  | loss 5.66  | Δw: 4.005\n",
            "it: 1900  | loss 5.58  | Δw: 3.739\n",
            "it: 1910  | loss 5.63  | Δw: 3.685\n",
            "it: 1920  | loss 5.76  | Δw: 3.573\n",
            "it: 1930  | loss 5.7  | Δw: 4.0\n",
            "it: 1940  | loss 5.65  | Δw: 3.85\n",
            "it: 1950  | loss 5.67  | Δw: 3.687\n",
            "it: 1960  | loss 5.64  | Δw: 3.617\n",
            "it: 1970  | loss 5.67  | Δw: 3.89\n",
            "it: 1980  | loss 5.56  | Δw: 4.054\n",
            "it: 1990  | loss 5.66  | Δw: 3.957\n",
            "it: 2000  | loss 5.62  | Δw: 3.799\n",
            "it: 2010  | loss 5.56  | Δw: 3.8\n",
            "it: 2020  | loss 5.66  | Δw: 4.023\n",
            "it: 2030  | loss 5.58  | Δw: 4.134\n",
            "it: 2040  | loss 5.62  | Δw: 4.151\n",
            "it: 2050  | loss 5.62  | Δw: 4.071\n",
            "it: 2060  | loss 5.58  | Δw: 3.872\n",
            "it: 2070  | loss 5.73  | Δw: 4.28\n",
            "it: 2080  | loss 5.61  | Δw: 3.864\n",
            "it: 2090  | loss 5.5  | Δw: 4.344\n",
            "it: 2100  | loss 5.54  | Δw: 4.078\n",
            "it: 2110  | loss 5.58  | Δw: 4.154\n",
            "it: 2120  | loss 5.53  | Δw: 4.227\n",
            "it: 2130  | loss 5.49  | Δw: 4.041\n",
            "it: 2140  | loss 5.45  | Δw: 4.112\n",
            "it: 2150  | loss 5.42  | Δw: 4.485\n",
            "it: 2160  | loss 5.52  | Δw: 4.482\n",
            "it: 2170  | loss 5.56  | Δw: 4.144\n",
            "it: 2180  | loss 5.58  | Δw: 4.111\n",
            "it: 2190  | loss 5.47  | Δw: 4.326\n",
            "it: 2200  | loss 5.4  | Δw: 4.376\n",
            "it: 2210  | loss 5.51  | Δw: 4.309\n",
            "it: 2220  | loss 5.63  | Δw: 4.351\n",
            "it: 2230  | loss 5.45  | Δw: 4.588\n",
            "it: 2240  | loss 5.44  | Δw: 4.313\n",
            "it: 2250  | loss 5.45  | Δw: 4.456\n",
            "it: 2260  | loss 5.44  | Δw: 4.476\n",
            "it: 2270  | loss 5.47  | Δw: 4.337\n",
            "it: 2280  | loss 5.42  | Δw: 4.429\n",
            "it: 2290  | loss 5.42  | Δw: 4.377\n",
            "it: 2300  | loss 5.39  | Δw: 4.608\n",
            "it: 2310  | loss 5.39  | Δw: 4.348\n",
            "it: 2320  | loss 5.45  | Δw: 4.56\n",
            "it: 2330  | loss 5.4  | Δw: 4.514\n",
            "it: 2340  | loss 5.38  | Δw: 4.644\n",
            "it: 2350  | loss 5.52  | Δw: 4.664\n",
            "it: 2360  | loss 5.53  | Δw: 4.686\n",
            "it: 2370  | loss 5.4  | Δw: 4.522\n",
            "it: 2380  | loss 5.46  | Δw: 4.805\n",
            "it: 2390  | loss 5.4  | Δw: 4.564\n",
            "it: 2400  | loss 5.37  | Δw: 4.355\n",
            "it: 2410  | loss 5.46  | Δw: 4.785\n",
            "it: 2420  | loss 5.4  | Δw: 4.821\n",
            "it: 2430  | loss 5.42  | Δw: 4.669\n",
            "it: 2440  | loss 5.32  | Δw: 4.663\n",
            "it: 2450  | loss 5.48  | Δw: 4.805\n",
            "it: 2460  | loss 5.33  | Δw: 4.863\n",
            "it: 2470  | loss 5.31  | Δw: 4.573\n",
            "it: 2480  | loss 5.37  | Δw: 4.792\n",
            "it: 2490  | loss 5.37  | Δw: 4.803\n",
            "it: 2500  | loss 5.36  | Δw: 4.888\n",
            "it: 2510  | loss 5.46  | Δw: 4.816\n",
            "it: 2520  | loss 5.38  | Δw: 4.813\n",
            "it: 2530  | loss 5.27  | Δw: 4.932\n",
            "it: 2540  | loss 5.34  | Δw: 4.682\n",
            "it: 2550  | loss 5.45  | Δw: 5.086\n",
            "it: 2560  | loss 5.36  | Δw: 4.808\n",
            "it: 2570  | loss 5.48  | Δw: 4.893\n",
            "it: 2580  | loss 5.23  | Δw: 4.973\n",
            "it: 2590  | loss 5.34  | Δw: 4.924\n",
            "it: 2600  | loss 5.37  | Δw: 5.096\n",
            "it: 2610  | loss 5.24  | Δw: 4.994\n",
            "it: 2620  | loss 5.26  | Δw: 5.198\n",
            "it: 2630  | loss 5.33  | Δw: 5.22\n",
            "it: 2640  | loss 5.34  | Δw: 5.07\n",
            "it: 2650  | loss 5.24  | Δw: 4.95\n",
            "it: 2660  | loss 5.3  | Δw: 5.147\n",
            "it: 2670  | loss 5.19  | Δw: 5.108\n",
            "it: 2680  | loss 5.12  | Δw: 4.942\n",
            "it: 2690  | loss 5.24  | Δw: 5.041\n",
            "it: 2700  | loss 5.21  | Δw: 5.105\n",
            "it: 2710  | loss 5.37  | Δw: 5.038\n",
            "it: 2720  | loss 5.3  | Δw: 5.277\n",
            "it: 2730  | loss 5.31  | Δw: 5.099\n",
            "it: 2740  | loss 5.22  | Δw: 5.105\n",
            "it: 2750  | loss 5.32  | Δw: 4.985\n",
            "it: 2760  | loss 5.18  | Δw: 5.257\n",
            "it: 2770  | loss 5.33  | Δw: 5.174\n",
            "it: 2780  | loss 5.15  | Δw: 5.379\n",
            "it: 2790  | loss 5.23  | Δw: 5.417\n",
            "it: 2800  | loss 5.15  | Δw: 5.292\n",
            "it: 2810  | loss 5.23  | Δw: 5.502\n",
            "it: 2820  | loss 5.16  | Δw: 5.286\n",
            "it: 2830  | loss 5.08  | Δw: 5.46\n",
            "it: 2840  | loss 5.18  | Δw: 5.564\n",
            "it: 2850  | loss 5.25  | Δw: 5.487\n",
            "it: 2860  | loss 5.15  | Δw: 5.396\n",
            "it: 2870  | loss 5.18  | Δw: 5.262\n",
            "it: 2880  | loss 5.11  | Δw: 5.091\n",
            "it: 2890  | loss 5.21  | Δw: 5.34\n",
            "it: 2900  | loss 5.22  | Δw: 5.757\n",
            "it: 2910  | loss 5.11  | Δw: 5.103\n",
            "it: 2920  | loss 5.1  | Δw: 5.408\n",
            "it: 2930  | loss 5.15  | Δw: 5.581\n",
            "it: 2940  | loss 5.22  | Δw: 5.262\n",
            "it: 2950  | loss 5.19  | Δw: 5.324\n",
            "it: 2960  | loss 5.24  | Δw: 5.566\n",
            "it: 2970  | loss 5.19  | Δw: 5.547\n",
            "it: 2980  | loss 5.15  | Δw: 5.362\n",
            "it: 2990  | loss 5.16  | Δw: 5.43\n",
            "it: 3000  | loss 5.25  | Δw: 5.392\n",
            "it: 3010  | loss 5.14  | Δw: 5.516\n",
            "it: 3020  | loss 5.2  | Δw: 5.462\n",
            "it: 3030  | loss 5.27  | Δw: 5.222\n",
            "it: 3040  | loss 5.08  | Δw: 5.661\n",
            "it: 3050  | loss 5.08  | Δw: 5.316\n",
            "it: 3060  | loss 5.11  | Δw: 5.721\n",
            "it: 3070  | loss 5.13  | Δw: 5.376\n",
            "it: 3080  | loss 5.11  | Δw: 5.851\n",
            "it: 3090  | loss 5.16  | Δw: 5.595\n",
            "it: 3100  | loss 5.15  | Δw: 5.674\n",
            "it: 3110  | loss 5.03  | Δw: 5.761\n",
            "it: 3120  | loss 5.04  | Δw: 5.661\n",
            "it: 3130  | loss 5.08  | Δw: 5.669\n",
            "it: 3140  | loss 5.14  | Δw: 5.775\n",
            "it: 3150  | loss 5.11  | Δw: 5.466\n",
            "it: 3160  | loss 5.1  | Δw: 5.768\n",
            "it: 3170  | loss 5.11  | Δw: 5.735\n",
            "it: 3180  | loss 5.15  | Δw: 5.531\n",
            "it: 3190  | loss 5.1  | Δw: 5.689\n",
            "it: 3200  | loss 5.24  | Δw: 5.572\n",
            "it: 3210  | loss 5.13  | Δw: 5.636\n",
            "it: 3220  | loss 5.03  | Δw: 5.509\n",
            "it: 3230  | loss 5.21  | Δw: 5.544\n",
            "it: 3240  | loss 5.23  | Δw: 5.845\n",
            "it: 3250  | loss 5.11  | Δw: 5.706\n",
            "it: 3260  | loss 5.08  | Δw: 5.835\n",
            "it: 3270  | loss 5.11  | Δw: 5.766\n",
            "it: 3280  | loss 5.1  | Δw: 5.64\n",
            "it: 3290  | loss 5.03  | Δw: 5.802\n",
            "it: 3300  | loss 5.05  | Δw: 5.748\n",
            "it: 3310  | loss 5.07  | Δw: 5.83\n",
            "it: 3320  | loss 5.08  | Δw: 5.602\n",
            "it: 3330  | loss 5.14  | Δw: 5.749\n",
            "it: 3340  | loss 5.05  | Δw: 5.806\n",
            "it: 3350  | loss 5.02  | Δw: 5.931\n",
            "it: 3360  | loss 5.09  | Δw: 5.78\n",
            "it: 3370  | loss 5.05  | Δw: 5.832\n",
            "it: 3380  | loss 5.08  | Δw: 5.743\n",
            "it: 3390  | loss 5.14  | Δw: 5.802\n",
            "it: 3400  | loss 5.06  | Δw: 5.964\n",
            "it: 3410  | loss 4.94  | Δw: 6.118\n",
            "it: 3420  | loss 5.03  | Δw: 5.933\n",
            "it: 3430  | loss 5.09  | Δw: 6.118\n",
            "it: 3440  | loss 5.07  | Δw: 5.776\n",
            "it: 3450  | loss 4.87  | Δw: 6.269\n",
            "it: 3460  | loss 5.1  | Δw: 6.009\n",
            "it: 3470  | loss 4.98  | Δw: 6.141\n",
            "it: 3480  | loss 5.06  | Δw: 6.166\n",
            "it: 3490  | loss 4.97  | Δw: 6.014\n",
            "it: 3500  | loss 5.06  | Δw: 5.811\n",
            "it: 3510  | loss 4.99  | Δw: 5.79\n",
            "it: 3520  | loss 5.04  | Δw: 6.056\n",
            "it: 3530  | loss 5.06  | Δw: 6.105\n",
            "it: 3540  | loss 4.99  | Δw: 5.99\n",
            "it: 3550  | loss 5.03  | Δw: 6.11\n",
            "it: 3560  | loss 4.93  | Δw: 5.703\n",
            "it: 3570  | loss 5.04  | Δw: 5.841\n",
            "it: 3580  | loss 5.11  | Δw: 6.091\n",
            "it: 3590  | loss 4.95  | Δw: 6.108\n",
            "it: 3600  | loss 4.92  | Δw: 6.086\n",
            "it: 3610  | loss 5.01  | Δw: 6.037\n",
            "it: 3620  | loss 4.92  | Δw: 5.85\n",
            "it: 3630  | loss 4.97  | Δw: 6.076\n",
            "it: 3640  | loss 4.97  | Δw: 5.916\n",
            "it: 3650  | loss 4.96  | Δw: 6.114\n",
            "it: 3660  | loss 4.92  | Δw: 6.113\n",
            "it: 3670  | loss 4.88  | Δw: 5.671\n",
            "it: 3680  | loss 4.85  | Δw: 5.986\n",
            "it: 3690  | loss 4.9  | Δw: 6.134\n",
            "it: 3700  | loss 4.93  | Δw: 6.136\n",
            "it: 3710  | loss 4.92  | Δw: 5.976\n",
            "it: 3720  | loss 4.95  | Δw: 6.024\n",
            "it: 3730  | loss 4.93  | Δw: 6.104\n",
            "it: 3740  | loss 4.94  | Δw: 6.179\n",
            "it: 3750  | loss 5.01  | Δw: 6.328\n",
            "it: 3760  | loss 5.06  | Δw: 6.092\n",
            "it: 3770  | loss 4.91  | Δw: 5.991\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ca8acdccca7c>\u001b[0m in \u001b[0;36m<cell line: 280>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;31m#get batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;31m#infer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ca8acdccca7c>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(loader, loader_iter)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ca8acdccca7c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index, p_random_mask)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         return {'input': torch.Tensor([w[0] for w in s]).long(),\n\u001b[0;32m--> 183\u001b[0;31m                 'target': torch.Tensor([w[1] for w in s]).long()}\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m#return length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Libs\n",
        "# =============================================================================\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from os.path import exists\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import math\n",
        "import re\n",
        "\n",
        "from models.transformer import Transformer\n",
        "\n",
        "\"\"\"\n",
        "# =============================================================================\n",
        "# Transformer\n",
        "# =============================================================================\n",
        "def attention(q, k, v, mask = None, dropout = None):\n",
        "    scores = q.matmul(k.transpose(-2, -1))\n",
        "    scores /= math.sqrt(q.shape[-1])\n",
        "\n",
        "    #mask\n",
        "    scores = scores if mask is None else scores.masked_fill(mask == 0, -1e3)\n",
        "\n",
        "    scores = F.softmax(scores, dim = -1)\n",
        "    scores = dropout(scores) if dropout is not None else scores\n",
        "    output = scores.matmul(v)\n",
        "    return output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, out_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "#        self.q_linear = nn.Linear(out_dim, out_dim)\n",
        "#        self.k_linear = nn.Linear(out_dim, out_dim)\n",
        "#        self.v_linear = nn.Linear(out_dim, out_dim)\n",
        "        self.linear = nn.Linear(out_dim, out_dim*3)\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.out_dim = out_dim\n",
        "        self.out_dim_per_head = out_dim // n_heads\n",
        "        self.out = nn.Linear(out_dim, out_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def split_heads(self, t):\n",
        "        return t.reshape(t.shape[0], -1, self.n_heads, self.out_dim_per_head)\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        #in decoder, y comes from encoder. In encoder, y=x\n",
        "        y = x if y is None else y\n",
        "\n",
        "        qkv = self.linear(x) # BS * SEQ_LEN * (3*EMBED_SIZE_L)\n",
        "        q = qkv[:, :, :self.out_dim] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        k = qkv[:, :, self.out_dim:self.out_dim*2] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        v = qkv[:, :, self.out_dim*2:] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "\n",
        "        #break into n_heads\n",
        "        q, k, v = [self.split_heads(t) for t in (q,k,v)]  # BS * SEQ_LEN * HEAD * EMBED_SIZE_P_HEAD\n",
        "        q, k, v = [t.transpose(1,2) for t in (q,k,v)]  # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
        "\n",
        "        #n_heads => attention => merge the heads => mix information\n",
        "        scores = attention(q, k, v, mask, self.dropout) # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
        "        scores = scores.transpose(1,2).contiguous().view(scores.shape[0], -1, self.out_dim) # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        out = self.out(scores)  # BS * SEQ_LEN * EMBED_SIZE\n",
        "\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, inp_dim, inner_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(inp_dim, inner_dim)\n",
        "        self.linear2 = nn.Linear(inner_dim, inp_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #inp => inner => relu => dropout => inner => inp\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, n_heads, inner_transformer_size, inner_ff_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(n_heads, inner_transformer_size, dropout)\n",
        "        self.ff = FeedForward(inner_transformer_size, inner_ff_size, dropout)\n",
        "        self.norm1 = nn.LayerNorm(inner_transformer_size)\n",
        "        self.norm2 = nn.LayerNorm(inner_transformer_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x2 = self.norm1(x)\n",
        "        x = x + self.dropout1(self.mha(x2, mask=mask))\n",
        "        x2 = self.norm2(x)\n",
        "        x = x + self.dropout2(self.ff(x2))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, n_code, n_heads, embed_size, inner_ff_size, n_embeddings, seq_len, dropout=.1):\n",
        "        super().__init__()\n",
        "\n",
        "        #model input\n",
        "        self.embeddings = nn.Embedding(n_embeddings, embed_size)\n",
        "        self.pe = PositionalEmbedding(embed_size, seq_len)\n",
        "\n",
        "        #backbone\n",
        "        encoders = []\n",
        "        for i in range(n_code):\n",
        "            encoders += [EncoderLayer(n_heads, embed_size, inner_ff_size, dropout)]\n",
        "        self.encoders = nn.ModuleList(encoders)\n",
        "\n",
        "        #language model\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.linear = nn.Linear(embed_size, n_embeddings, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x = x + self.pe(x)\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "# Positional Embedding\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len = 80):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        pe.requires_grad = False\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:,:x.size(1)] #x.size(1) = seq_len\n",
        "\"\"\"\n",
        "# =============================================================================\n",
        "# Dataset\n",
        "# =============================================================================\n",
        "class SentencesDataset(Dataset):\n",
        "    #Init dataset\n",
        "    def __init__(self, sentences, vocab, seq_len):\n",
        "        dataset = self\n",
        "\n",
        "        dataset.sentences = sentences\n",
        "        dataset.vocab = vocab + ['<ignore>', '<oov>', '<mask>']\n",
        "        dataset.vocab = {e:i for i, e in enumerate(dataset.vocab)}\n",
        "        dataset.rvocab = {v:k for k,v in dataset.vocab.items()}\n",
        "        dataset.seq_len = seq_len\n",
        "\n",
        "        #special tags\n",
        "        dataset.IGNORE_IDX = dataset.vocab['<ignore>'] #replacement tag for tokens to ignore\n",
        "        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>'] #replacement tag for unknown words\n",
        "        dataset.MASK_IDX = dataset.vocab['<mask>'] #replacement tag for the masked word prediction task\n",
        "\n",
        "\n",
        "    #fetch data\n",
        "    def __getitem__(self, index, p_random_mask=0.15):\n",
        "        dataset = self\n",
        "\n",
        "        #while we don't have enough word to fill the sentence for a batch\n",
        "        s = []\n",
        "        while len(s) < dataset.seq_len:\n",
        "            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n",
        "            index += 1\n",
        "\n",
        "        #ensure that the sequence is of length seq_len\n",
        "        s = s[:dataset.seq_len]\n",
        "        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))] #PAD ok\n",
        "\n",
        "        #apply random mask\n",
        "        s = [(dataset.MASK_IDX, w) if random.random() < p_random_mask else (w, dataset.IGNORE_IDX) for w in s]\n",
        "\n",
        "        return {'input': torch.Tensor([w[0] for w in s]).long(),\n",
        "                'target': torch.Tensor([w[1] for w in s]).long()}\n",
        "\n",
        "    #return length\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    #get words id\n",
        "    def get_sentence_idx(self, index):\n",
        "        dataset = self\n",
        "        s = dataset.sentences[index]\n",
        "        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s]\n",
        "        return s\n",
        "\n",
        "# =============================================================================\n",
        "# Methods / Class\n",
        "# =============================================================================\n",
        "def get_batch(loader, loader_iter):\n",
        "    try:\n",
        "        batch = next(loader_iter)\n",
        "    except StopIteration:\n",
        "        loader_iter = iter(loader)\n",
        "        batch = next(loader_iter)\n",
        "    return batch, loader_iter\n",
        "\n",
        "# =============================================================================\n",
        "# #Init\n",
        "# =============================================================================\n",
        "print('initializing..')\n",
        "batch_size = 1024\n",
        "seq_len = 20\n",
        "embed_size = 128\n",
        "inner_ff_size = embed_size * 4\n",
        "n_heads = 8\n",
        "n_code = 8\n",
        "n_vocab = 40000\n",
        "dropout = 0.1\n",
        "# n_workers = 12\n",
        "\n",
        "#optimizer\n",
        "optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}\n",
        "\n",
        "# =============================================================================\n",
        "# Input\n",
        "# =============================================================================\n",
        "#1) load text\n",
        "print('loading text...')\n",
        "pth = 'training.txt'\n",
        "sentences = open(pth).read().lower().split('\\n')\n",
        "\n",
        "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
        "print('tokenizing sentences...')\n",
        "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
        "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
        "sentences = [[w for w in s if len(w)] for s in sentences]\n",
        "\n",
        "#3) create vocab if not already created\n",
        "print('creating/loading vocab...')\n",
        "pth = 'vocab.txt'\n",
        "if not exists(pth):\n",
        "    words = [w for s in sentences for w in s]\n",
        "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
        "    vocab = [w[0] for w in vocab]\n",
        "    open(pth, 'w+').write('\\n'.join(vocab))\n",
        "else:\n",
        "    vocab = open(pth).read().split('\\n')\n",
        "\n",
        "#4) create dataset\n",
        "print('creating dataset...')\n",
        "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
        "# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Model\n",
        "# =============================================================================\n",
        "#init model\n",
        "print('initializing model...')\n",
        "model = Transformer(n_code=n_code,n_heads= n_heads,embed_size= embed_size,inner_ff_size= inner_ff_size,n_embeddings= len(dataset.vocab), seq_len=seq_len, dropout=dropout, model_type='BERT')\n",
        "model = model.cuda()\n",
        "\n",
        "# =============================================================================\n",
        "# Optimizer\n",
        "# =============================================================================\n",
        "print('initializing optimizer and loss...')\n",
        "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
        "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)\n",
        "\n",
        "# =============================================================================\n",
        "# Train\n",
        "# =============================================================================\n",
        "print('training...')\n",
        "print_each = 10\n",
        "model.train()\n",
        "batch_iter = iter(data_loader)\n",
        "n_iteration = 10000\n",
        "for it in range(n_iteration):\n",
        "\n",
        "    #get batch\n",
        "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
        "\n",
        "    #infer\n",
        "    masked_input = batch['input']\n",
        "    masked_target = batch['target']\n",
        "\n",
        "    masked_input = masked_input.cuda(non_blocking=True)\n",
        "    masked_target = masked_target.cuda(non_blocking=True)\n",
        "    output = model(masked_input)\n",
        "\n",
        "    #compute the cross entropy loss\n",
        "    output_v = output.view(-1,output.shape[-1])\n",
        "    target_v = masked_target.view(-1,1).squeeze()\n",
        "    loss = loss_model(output_v, target_v)\n",
        "\n",
        "    #compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    #apply gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    #print step\n",
        "    if it % print_each == 0:\n",
        "        print('it:', it,\n",
        "              ' | loss', np.round(loss.item(),2),\n",
        "              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
        "\n",
        "    #reset gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Results analysis\n",
        "# =============================================================================\n",
        "print('saving embeddings...')\n",
        "N = 3000\n",
        "np.savetxt('values.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n",
        "s = [dataset.rvocab[i] for i in range(N)]\n",
        "open('names.tsv', 'w+').write('\\n'.join(s) )\n",
        "\n",
        "\n",
        "print('end')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzO5Ml9PB58Y"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b8fbfcbe0e544000e4ba3d2d9974592a7ba1a2af52205db5302ae41a0c45d995"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}